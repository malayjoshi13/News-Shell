{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from nltk) (2022.7.25)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (4.21.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (2022.7.25)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from transformers) (2.28.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: wordfreq in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: regex>=2020.04.04 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from wordfreq) (2022.7.25)\n",
      "Requirement already satisfied: langcodes>=3.0 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from wordfreq) (3.3.0)\n",
      "Requirement already satisfied: ftfy>=6.1 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from wordfreq) (6.1.1)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from wordfreq) (1.0.4)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from ftfy>=6.1->wordfreq) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "! pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from keras.models import load_model\n",
    "!pip install torch\n",
    "import torch\n",
    "!pip install transformers\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "!pip install wordfreq\n",
    "from wordfreq import zipf_frequency\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\sih\\News-Shell\\server\\src\\index.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/sih/News-Shell/server/src/index.ipynb#ch0000005?line=3'>4</a>\u001b[0m bert_model \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbert-large-uncased\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/sih/News-Shell/server/src/index.ipynb#ch0000005?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(bert_model)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/sih/News-Shell/server/src/index.ipynb#ch0000005?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m BertForMaskedLM\u001b[39m.\u001b[39;49mfrom_pretrained(bert_model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/sih/News-Shell/server/src/index.ipynb#ch0000005?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/sih/News-Shell/server/src/index.ipynb#ch0000005?line=8'>9</a>\u001b[0m \u001b[39m# Generate candidates\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\modeling_utils.py:2106\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2103\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2105\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2106\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2108\u001b[0m \u001b[39mif\u001b[39;00m device_map \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   2109\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39m_no_split_modules \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1306\u001b[0m, in \u001b[0;36mBertForMaskedLM.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mis_decoder:\n\u001b[0;32m   1301\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1303\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbi-directional self-attention.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1304\u001b[0m     )\n\u001b[1;32m-> 1306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert \u001b[39m=\u001b[39m BertModel(config, add_pooling_layer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m   1307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls \u001b[39m=\u001b[39m BertOnlyMLMHead(config)\n\u001b[0;32m   1309\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:888\u001b[0m, in \u001b[0;36mBertModel.__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m    887\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m BertEmbeddings(config)\n\u001b[1;32m--> 888\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m BertEncoder(config)\n\u001b[0;32m    890\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39m=\u001b[39m BertPooler(config) \u001b[39mif\u001b[39;00m add_pooling_layer \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:556\u001b[0m, in \u001b[0;36mBertEncoder.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    555\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m--> 556\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BertLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    557\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:556\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    555\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m--> 556\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BertLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    557\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:478\u001b[0m, in \u001b[0;36mBertLayer.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m should be used as a decoder model if cross attention is added\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    477\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossattention \u001b[39m=\u001b[39m BertAttention(config, position_embedding_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 478\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate \u001b[39m=\u001b[39m BertIntermediate(config)\n\u001b[0;32m    479\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m BertOutput(config)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertIntermediate.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[0;32m    439\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(config\u001b[39m.\u001b[39;49mhidden_size, config\u001b[39m.\u001b[39;49mintermediate_size)\n\u001b[0;32m    441\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config\u001b[39m.\u001b[39mhidden_act, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    442\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn \u001b[39m=\u001b[39m ACT2FN[config\u001b[39m.\u001b[39mhidden_act]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[0;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[1;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[0;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Candidates generation using BERT and selection of best candidates based on Zipf values\n",
    "\n",
    "# Load the BERT  model for masked languge\n",
    "bert_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertForMaskedLM.from_pretrained(bert_model)\n",
    "model.eval()\n",
    "\n",
    "# Generate candidates\n",
    "def get_bert_candidates(input_text, list_cwi_predictions, numb_predictions_displayed = 10):\n",
    "  list_candidates_bert = []\n",
    "  for word,pred  in zip(input_text.split(), list_cwi_predictions):\n",
    "    if (pred and (pos_tag([word])[0][1] in ['NNS', 'NN', 'VBP', 'RB', 'VBG','VBD' ]))  or (zipf_frequency(word, 'en')) <3.1:\n",
    "      replace_word_mask = input_text.replace(word, '[MASK]')\n",
    "      text = f'[CLS]{replace_word_mask} [SEP] {input_text} [SEP] '\n",
    "      tokenized_text = tokenizer.tokenize(text)\n",
    "      masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
    "      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "      segments_ids = [0]*len(tokenized_text)\n",
    "      tokens_tensor = torch.tensor([indexed_tokens])\n",
    "      segments_tensors = torch.tensor([segments_ids])\n",
    "      # Predict all tokens\n",
    "      with torch.no_grad():\n",
    "          outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "          predictions = outputs[0][0][masked_index]\n",
    "      predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\n",
    "      predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
    "      list_candidates_bert.append((word, predicted_tokens))\n",
    "  return list_candidates_bert\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "# Functions that will help prepare raw data before getting simplified\n",
    "\n",
    "# Function to clean the data and remove non characters symbols\n",
    "stop_words_ = set(stopwords.words('english'))\n",
    "def cleaner(word):\n",
    "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', '', word, flags=re.MULTILINE)\n",
    "  word = re.sub('[\\W]', ' ', word)\n",
    "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
    "  return word.lower().strip()\n",
    "\n",
    "\n",
    "filename = 'word_to_index.pkl'\n",
    "word2index = pickle.load(open(filename, 'rb'))\n",
    "sent_max_length = 103\n",
    "\n",
    "# Function to create the padded sequence\n",
    "def process_input(input_text):\n",
    "  input_text = cleaner(input_text)\n",
    "  clean_text = []\n",
    "  index_list =[]\n",
    "  input_token = []\n",
    "  index_list_zipf = []\n",
    "  for i, word in enumerate(input_text.split()):\n",
    "    if word in word2index:\n",
    "      clean_text.append(word)\n",
    "      input_token.append(word2index[word])\n",
    "    else:\n",
    "      index_list.append(i)\n",
    "  input_padded = pad_sequences(maxlen=sent_max_length, sequences=[input_token], padding=\"post\", value=0)\n",
    "  return input_padded, index_list, len(clean_text)  \n",
    "\n",
    "def complete_missing_word(pred_binary, index_list, len_list):\n",
    "  list_cwi_predictions = list(pred_binary[0][:len_list])\n",
    "  for i in index_list:\n",
    "    list_cwi_predictions.insert(i, 0)\n",
    "  return list_cwi_predictions  \n",
    "\n",
    "model_cwi = load_model(\"model_CWI_full.h5\")\n",
    "\n",
    "# Main function that simplifies text\n",
    "def simplify_it(input_text):\n",
    "  new_text = input_text\n",
    "  input_padded, index_list, len_list = process_input(input_text)\n",
    "  pred_cwi = model_cwi.predict(input_padded)\n",
    "  pred_cwi_binary = np.argmax(pred_cwi, axis = 2)\n",
    "  complete_cwi_predictions = complete_missing_word(pred_cwi_binary, index_list, len_list)\n",
    "  bert_candidates =   get_bert_candidates(input_text, complete_cwi_predictions)\n",
    "  for word_to_replace, l_candidates in bert_candidates:\n",
    "    tuples_word_zipf = []\n",
    "    for w in l_candidates:\n",
    "      if w.isalpha():\n",
    "        tuples_word_zipf.append((w, zipf_frequency(w, 'en')))\n",
    "    tuples_word_zipf = sorted(tuples_word_zipf, key = lambda x: x[1], reverse=True)\n",
    "    new_text = re.sub(word_to_replace, tuples_word_zipf[0][0], new_text) \n",
    "\n",
    "  return new_text\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------- \n",
    "\n",
    "# driver code\n",
    "if __name__ == \"__main__\":\n",
    "  input_text = 'Pro-democracy parties, which had hoped to ride widespread discontent to big gains, saw the yearlong delay as an attempt to thwart them.'\n",
    "  simplified_text = simplify_it(input_text)\n",
    "  print(\"Original text: \", input_text )\n",
    "  print(\"Simplified text:\", simplified_text)\n",
    "  print(simplified_text)\n",
    "  sys.stdout.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dc6fe3c0647c88d90a7f8b3bd4f2ae3f37f883cf89f4bfe059b22d4885ce559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
